# LLM (provider: openai | grok | gemini | huggingface)
LLM_PROVIDER=openai
OPENAI_API_KEY=your-openai-api-key
LLM_MODEL=gpt-4-turbo-preview
TEMPERATURE=0.7

# Grok (xAI): set LLM_PROVIDER=grok, LLM_MODEL=grok-2
# XAI_API_KEY=your-xai-api-key

# Gemini: set LLM_PROVIDER=gemini, LLM_MODEL=gemini-1.5-pro
# GOOGLE_API_KEY=your-google-api-key

# Hugging Face (e.g. Qwen): set LLM_PROVIDER=huggingface, LLM_MODEL=Qwen/Qwen2.5-7B-Instruct
# HUGGINGFACE_API_KEY=your-huggingface-token

EMBEDDING_MODEL=text-embedding-3-small

# Embeddings (openai | sentence_transformers)
EMBEDDINGS_PROVIDER=openai
SENTENCE_TRANSFORMER_MODEL=all-MiniLM-L6-v2

# RAG
CHROMA_PERSIST_DIR=./data/chroma_db
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
# Chunking: recursive_character | sentence
CHUNKING_STRATEGY=recursive_character
# Hybrid search (vector + BM25)
RAG_HYBRID_SEARCH=false
# Rerank: 0 = off, else retrieve top_k * this then rerank to top_k (needs sentence-transformers)
RAG_RERANK_TOP_N=0

# Observability
ENABLE_LLM_TRACING=false
TRACING_LOG_LEVEL=INFO

# API
SECRET_KEY=your-secret-key
DEBUG=True

# Documents
UPLOAD_DIR=./uploads
MAX_UPLOAD_SIZE=10485760

# MCP (optional): stdio server command and args (JSON array)
# MCP_COMMAND=python
# MCP_ARGS=["path/to/mcp_server.py"]

# Task queue (Celery): optional, for batch RAG, batch bills, agent runs
# CELERY_BROKER_URL=redis://localhost:6379/0
# CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Prompts: versioned prompts base path (file-based)
PROMPTS_BASE_PATH=./data/prompts
# A/B test default metric: exact_match | keyword_match | latency
PROMPT_AB_METRIC=keyword_match

# Evaluation: golden datasets and feedback
GOLDEN_DATASETS_PATH=./data/golden
FEEDBACK_STORE_PATH=./data/feedback/feedback.jsonl

# Vector store: chroma | pinecone | weaviate | qdrant | pgvector (default chroma)
VECTOR_STORE=chroma
# Pinecone (set VECTOR_STORE=pinecone): pip install gen-ai-framework[vectorstore-pinecone]
# PINECONE_API_KEY=...
# PINECONE_INDEX_NAME=...
# PINECONE_ENV=...
# Weaviate: VECTOR_STORE=weaviate, WEAVIATE_URL=http://localhost:8080
# Qdrant: VECTOR_STORE=qdrant, QDRANT_URL=http://localhost:6333
# pgvector (Postgres): VECTOR_STORE=pgvector, PGVECTOR_CONNECTION_STRING=postgresql://...

# Confluence (optional): ingest pages into RAG. pip install -e ".[confluence]"
# CONFLUENCE_BASE_URL=https://your-site.atlassian.net/wiki
# Cloud: CONFLUENCE_EMAIL=you@company.com, CONFLUENCE_API_TOKEN=your-api-token
# Server/DC: CONFLUENCE_USER=username, CONFLUENCE_PASSWORD=password
